{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SImulating data\n",
    "\n",
    "In this step we simulate some data in the following form:\n",
    "\n",
    "$y_i = mx_i + b + \\epsilon_i$\n",
    "\n",
    "Where $\\epsilon_i$ is in the form:\n",
    "\n",
    "$\\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "For this case, we defined the following:\n",
    "\n",
    "$m=5 \\\\ b= 2 \\\\ \\sigma^2 = 9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "seed = [1,1,1,1,1,1,1]\n",
    "\n",
    "m = 5\n",
    "b = 2\n",
    "\n",
    "std = 3\n",
    "\n",
    "x = np.arange(0,11,1)\n",
    "X = [x, x**2, x**3, x**4, x**5, x**6]\n",
    "\n",
    "xForPred = np.arange(0,10.1,.1)\n",
    "XForPred = [xForPred, xForPred**2, xForPred**3, xForPred**4, xForPred**5, xForPred**6]\n",
    "\n",
    "\n",
    "epsilon = np.random.normal(0, std, (len(x))) \n",
    "\n",
    "y = m * x + b + epsilon\n",
    "\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "In this step we do a Linear Regression with the form:\n",
    "\n",
    "$y = mx + b$\n",
    "\n",
    "By minimizing the error function:\n",
    "\n",
    "$\\sum_{i=1}^n(y_i - mx_i + b)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def l_function(thetas):\n",
    "    return sum((y - np.dot(thetas[0:-1],X[0:1]) - thetas[-1])**2)\n",
    "\n",
    "res = minimize(l_function, seed[0:2], method='BFGS')\n",
    "\n",
    "ypred = res.x[0] * x + res.x[1] \n",
    "plt.scatter(x,y)\n",
    "plt.plot(ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "In this part we propose some hypotesis:\n",
    "\n",
    "$H_0^a: y_i = \\theta_1 x_1^i + \\theta_2 (x_1^i)^2 + \\theta_0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ha_function(thetas):\n",
    "    return sum((y - np.dot(thetas[0:-1],X[0:2]) - thetas[-1])**2)\n",
    "\n",
    "resHa = minimize(ha_function, seed[0:3], method='BFGS')\n",
    "\n",
    "ypredHa = np.dot(resHa.x[0:-1],XForPred[0:2]) + resHa.x[-1]\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xForPred, ypredHa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0^b: y_i = \\theta_1 x_1^i + \\theta_2 (x_1^i)^2 + \\theta_3 (x_1^i)^3 + \\theta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hb_function(thetas):\n",
    "    return sum((y - np.dot(thetas[0:-1],X[0:3]) - thetas[-1])**2)\n",
    "\n",
    "resHb = minimize(hb_function, seed[0:4], method='BFGS')\n",
    "\n",
    "ypredHb = np.dot(resHb.x[0:-1],XForPred[0:3]) + resHb.x[-1]\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xForPred, ypredHb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0^c: y_i = \\theta_1 x_1^i + \\theta_2 (x_1^i)^2 + \\theta_3 (x_1^i)^3 +\\theta_4 (x_1^i)^4 +\\theta_5 (x_1^i)^5 +\\theta_6 (x_1^i)^6 +\\theta_7 (x_1^i)^7 + \\theta_0$\n",
    "\n",
    "In this Hypotesis we can see that there is a lot of overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hc_function(thetas):\n",
    "    return sum((y - np.dot(thetas[0:-1],X) - thetas[-1])**2)\n",
    "\n",
    "resHc = minimize(hc_function, seed[0:7], method='BFGS')\n",
    "\n",
    "xForPred = np.arange(0,10.1,.1)\n",
    "\n",
    "ypredHc = np.dot(resHc.x[0:-1],XForPred) + resHc.x[-1]\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xForPred, ypredHc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regularization \n",
    "\n",
    "To avoid overfitting, we are going to regularize the last function by using Ridge regularization, this takes the form of:\n",
    "\n",
    "$\\sum(y_i - m_{\\theta}(x_i))^2 + \\lambda (\\theta_1^2 + \\theta_1^2 + .... + \\theta_n^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 100\n",
    "# Lambda por cada uno de los thetas^2\n",
    "def ridge_function(thetas):\n",
    "    return sum((y - np.dot(thetas[0:-1],X) - thetas[-1])**2) + lambd*(sum(thetas**2))\n",
    "\n",
    "resRidge = minimize(ridge_function, seed, method='BFGS')\n",
    "\n",
    "ypredRidge = np.dot(resRidge.x[0:-1],XForPred) + resRidge.x[-1]\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xForPred, ypredRidge)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizacion LASSO\n",
    "\n",
    "To avoid overfitting, we are going to regularize the last function by using Lasso regularization, this takes the form of:\n",
    "\n",
    "$\\sum(y_i - m_{\\theta}(x_i))^2 + \\lambda (|\\theta_1| + |\\theta_1| + .... + |\\theta_n|)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 100\n",
    "\n",
    "def lasso_function(thetas):\n",
    "    return sum((y - np.dot(thetas[0:-1],X) - thetas[-1])**2) + lambd * sum(abs(thetas))\n",
    "\n",
    "resLasso = minimize(lasso_function, seed, method='BFGS')\n",
    "\n",
    "ypredLasso = np.dot(resLasso.x[0:-1],XForPred) + resLasso.x[-1]\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xForPred,ypredLasso)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "In order to choose the lambda value we use the following process called k-fold cross validation:\n",
    "\n",
    "- Split the entire data randomly into k folds (value of k shouldn’t be too small or too high, ideally we choose 5 to 10 depending on the data size). The higher value of K leads to less biased model (but large variance might lead to overfit), where as the lower value of K is similar to the train-test split approach we saw before.\n",
    "- Then fit the model using the K — 1 (K minus 1) folds and validate the model using the remaining Kth fold. Note down the scores/errors.\n",
    "- Repeat this process until every K-fold serve as the test set. Then take the average of your recorded scores. That will be the performance metric for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
